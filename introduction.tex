\chapter{Introduction}
\lettrine{A}{}fter the discovery of the Higgs boson, many experiments at the Large Hadron Collider (LHC) focused on discovering beyond Standard Model (BSM) physiscs. Despite the SM of elementary particles is a common accepted and well established theory it has many open issues such the lacking of a Dark Matter candidate.
  
Principal theories proposing to solve this open issues having the common feature, of searching for BSM physics, are SUSY, large extra dimensions theories and Dark Matter theories in the form of Weakly Interacting Massive Particles (WIMPs). WIMP theory is an interesting proposal thanks to the so called ``WIMP miracle'' stating that a particle with weak coupling to SM with a mass in the \SI{1}{\gev}-\SI{100}{\gev} range can reproduct very well the relic density of DM measured by various experiments such the PLANCK mission by ESA. Great effort is accomplished today to get any evidence of Dark Matter. The search comprehends direct detection in which recoil of SM particles from eventual WIMPs is measured, indirect detection in which SM annihilation products of DM are looked for and colliders production in which DM comes from SM particle annihilation.

In this work a new model, called for its simplicity Minimal Dark Matter Model, is tested and interpreted in terms of a \mph analysis. This model simply adds a fermionic triplet to the SM, made of two opposite charged particles \chipm and a neutral one, which associates to an eventual WIMP its neutral particle \chizero. \chizero mass is the only parameter of this model and in this work several value of it are tested in order to set upper limits on this model with a \mph analysis.

The \mph analysis is one of the analysis carried out with the ATLAS detector at the LHC which provides in the final state of \pp collision a signature made by a single energetic photon and large missing transverse momentum (\met). Large missing momentum signature are striking clues of eventual new phenomena produced. On the other hand an inbalance of energy due to non-reconstructed objects cannot be detected alone, instead it must be tagged with a well defined object such a photon or a jet. These kind of analyses are called \emph{Mono-X}. The MonoJets exhibits the higer production rate, while the \mph has the cleanest final state: few other SM processes produces its same signature, making the \mph analysis to be less contaminated by discarded events. The analysis made was pursued following the lead of the one performed by the ATLAS Collaboration in 2017.

Data collected in 2015 and 2016 corresponding to an integrated luminosity of \SI{36.4}{\ifb} at a \cm energy of \SI{13}{\tev} are used. The main background sources extimated for the \mph analysis are: the double neutrino decay of the Z Boson, which is the dominant one, and its double lepton decay, the lepton + neutrino decay of the W Boson. In all the previous events a photon has to be radiated from the initial state. Morover all kind of events in which a photon could be faked by electrons or jets are taken into account. The analysis strategy involves the definition of a Signal Region (SR), which maximies the probability of finding signal over background events, and several Control Regions (CR) used to extimate the background sources in the SR via the ``Trasfer Factor'' techique which provides a parameter $k$ for every background process  acting as a normalization constant of MC events. No excess of signal are found in the SR and the results are in accordance with the expectation of the SM.
  
For the analysis, several sample masses are considered in order to generate Monte Carlo events.  The whole ATLAS production chain has been followed. It comprehends the generation of the events, the detector simulation, the digitization of the event and object reconstruction. The output of this process is an EXOT6 file containing only informations  and events useful for the \mph analysis. 

The analysis performes is likelihood based. A likelihood function which accounts for all informations in the various region is defined and test statistics involving $t_\mu$ and $\cls$ defined from the likelihood are used in order to evaluate the experimental sensitivity.

Results are interpreted in terms of, model independent, observed and expected exclusion limits on the visible cross section ($\sigma\times A\times\epsilon$) which, at \SI{95}{\percent} CL, are \SI{5.91}{fb} observed and \SI{8.86}{fb} expected. Then a fiducial region is defined to provide constraints on new physics in terms of any other possible signal model. From cuts defined in there a fiducial acceptance and a fiducial efficiency are computed and limits on excluded mass for the Minimal DM model are extracted. This analysis sets an upper limit on the \chizero mass of about \SI{48}{\gev}. Outlook at high luminosity are executed and the same procedure is applied. The computed upper limit at \SI{120}{\ifb} is \SI{96}{\gev} and at \SI{3000}{\ifb} is \SI{385}{\gev}.

Then the same procedure is applied to set upper limits on \chizero mass by using a model dependent approach starting from events in the signal region. The comuted upper limit is about \SI{61}{\gev}. The two results agree in the order of magnitude of the upper limit mass predicted.

\medskip
  
  Only for illustrative purposes, an overview of what could be found in this thesis is given. In chapter 1 the LHC facility at CERN and the ATLAS experiment are introduced, describing the detector and how it works. Chapter 2 provides a general overview on the Dark Matter problem and introduces the Minimal Dark Matter model. The ATLAS production chain with which the event sample were generated is described in Chapter 3. Fundamentals of \mph analysis are given in Chapter 4 and Chapter 5 reports the analysis result and the interpretation of the model tested with  the \mph analysis.



